{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f047ab",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/lukebarousse/Python_Data_Analytics_Course/blob/main/1_Basics/14_List_Comprehensions.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGv0RJ4-awf3"
   },
   "source": [
    "# List Comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* A way to create a new list (with shorter syntax) based on the values of an existing list.\n",
    "\n",
    "Not limited to only `list` comprehension: \n",
    "- `set` comprehension\n",
    "- `tuple` comprehension\n",
    "- `dictionary` comprehension\n",
    "\n",
    "## Importance\n",
    "\n",
    "Provide a concise way to create lists. Useful for data manipulation and filtering in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of numbers from 0 to 9\n",
    "numbers = [x for x in range(10)]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example # 1\n",
    "\n",
    "We're going to modify our example that we used in our `for` loop. Intsead of having the whole print statement with \"Position requires X years of experience\". We are just going to print out the experience required. This is a simplified version of our code earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1711426991765,
     "user": {
      "displayName": "Kelly Adams",
      "userId": "09437392635559217735"
     },
     "user_tz": 420
    },
    "id": "vEJN6KYx_25k",
    "outputId": "11d275ae-be83-40a6-b57c-71a6ce419beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Minimum experience required for job positions\n",
    "position_experience_requirements = [1, 2, 3]\n",
    "\n",
    "# Iterate over each experience requirement in the list of job positions\n",
    "for x in position_experience_requirements:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmKheGy0_31s"
   },
   "source": [
    "Now let's use list comprehension to shorten this.\n",
    "\n",
    "- The code defines `position_experience_requirements` as a list of integers representing minimum years of experience required for various job positions.\n",
    "- The for loop goes through each list item in `postion_experience_requirements` and prints out the `requirement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of job positions \n",
    "experience = [x for x in position_experience_requirements]\n",
    "\n",
    "# The result will be a list of job positions \n",
    "experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty basic. So let's make it a bit more useful. I'm going to add in a variable that stores the user's years of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_experience = 2\n",
    "user_experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are adding an if condition to our list comprehension. This condition checks if the user's experience (`user_experience`) is greater than or equal to each item (`x`) in the `position_experience_requirements` list.\n",
    "\n",
    "```python\n",
    "if user_experience >= x\n",
    "```\n",
    "\n",
    "It returns only the jobs where the requirement is met or is lower than the user's experience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of job positions for which the user is qualified\n",
    "    \n",
    "qualified_positions= [x for x in position_experience_requirements if user_experience>= x]\n",
    "\n",
    "qualified_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first code block extracts the data we need for this exercise; we'll dive into this later in the course.\n",
    "\n",
    "For now just understand I'm extracting the list of `job_titles` form our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "     -------------------------------------- 471.6/471.6 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\silva\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.12.0)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.1.2-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "     ---------------------------------------- 25.1/25.1 MB 4.7 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.9/64.9 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting xxhash (from datasets)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.8/134.8 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2024.6.1,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.6/177.6 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-win_amd64.whl (381 kB)\n",
      "     -------------------------------------- 381.1/381.1 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.22.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "     -------------------------------------- 436.6/436.6 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting packaging (from datasets)\n",
      "  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 54.0/54.0 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     -------------------------------------- 161.8/161.8 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 63.0/63.0 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "     ---------------------------------------- 50.4/50.4 kB ? eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.15.4-cp310-cp310-win_amd64.whl (85 kB)\n",
      "     ---------------------------------------- 85.1/85.1 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.22.0->datasets)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "     -------------------------------------- 102.2/102.2 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets)\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 70.4/70.4 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.3/126.3 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "     -------------------------------------- 167.3/167.3 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\silva\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.66.3->datasets) (0.4.4)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.8/134.8 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\silva\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "     -------------------------------------- 508.0/508.0 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "     -------------------------------------- 346.6/346.6 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\silva\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 45.0/45.0 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, pyyaml, propcache, packaging, numpy, idna, fsspec, frozenlist, dill, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, requests, pyarrow, pandas, multiprocess, multidict, aiosignal, yarl, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.2 idna-3.10 multidict-6.1.0 multiprocess-0.70.16 numpy-2.1.2 packaging-24.1 pandas-2.2.3 propcache-0.2.0 pyarrow-17.0.0 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 tqdm-4.66.5 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.15.4\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\silva\\.cache\\huggingface\\hub\\datasets--lukebarousse--data_jobs. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 785741/785741 [00:07<00:00, 111831.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('lukebarousse/data_jobs')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Create a list of job titles from the dataset\n",
    "job_list = df['job_title'].tolist()\n",
    "\n",
    "# Remove any non-string values from the list\n",
    "job_list = [job for job in job_list if isinstance(job, str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our previous `for` loop into a list comp!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Analyst',\n",
       " 'Stagiaire Data Analyst (H/F) - Lyon (69006)',\n",
       " 'Data Analyst',\n",
       " 'Senior Officer, Data Analyst, GTO',\n",
       " 'Stage - Data Analyst F/H',\n",
       " 'Data Analyst als Marketing Manager Automation (W/D/M)',\n",
       " 'Data Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Data Analyst (Bangkok Based, relocation provided)',\n",
       " 'Senior Data Analyst']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previous for loop\n",
    "analyst_list = []\n",
    "\n",
    "for job in job_list:\n",
    "  if \"Data Analyst\" in job:\n",
    "    analyst_list.append(job)\n",
    "\n",
    "# show first 10 values\n",
    "analyst_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However that was 4 lines of code! \n",
    "\n",
    "With list comprehension we can do it in only 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Analyst',\n",
       " 'Stagiaire Data Analyst (H/F) - Lyon (69006)',\n",
       " 'Data Analyst',\n",
       " 'Senior Officer, Data Analyst, GTO',\n",
       " 'Stage - Data Analyst F/H',\n",
       " 'Data Analyst als Marketing Manager Automation (W/D/M)',\n",
       " 'Data Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Data Analyst (Bangkok Based, relocation provided)',\n",
       " 'Senior Data Analyst']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_list = [job for job in job_list if \"Data Analyst\" in job]\n",
    "\n",
    "# show first 10 values\n",
    "analyst_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job list is:      785740 jobs\n",
      "Analyst list is:  162708 jobs\n"
     ]
    }
   ],
   "source": [
    "print(\"Job list is:     \" , len(job_list), \"jobs\")\n",
    "print(\"Analyst list is: \", len(analyst_list), \"jobs\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
